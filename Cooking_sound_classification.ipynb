{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. トレーニングデータ読み込み\n",
    "## 1. Load training dataset\n",
    "- kitchen20 dataset (https://github.com/marc-moreaux/kitchen20)\n",
    "  - \"chopping\"\n",
    "  - \"frying-pan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import IPython\n",
    "from scipy import signal\n",
    "from scipy.fft import fft\n",
    "from scipy.fftpack import fftfreq\n",
    "sr = 44100\n",
    "\n",
    "\n",
    "def load(path, sampling_rate):\n",
    "    wav_data = glob.glob(path)\n",
    "    wav_data = sorted(wav_data) \n",
    "    #print(wav_data)\n",
    "\n",
    "    sound_data_list = [i for i in range(len(wav_data))]\n",
    "    for i in range(len(wav_data)):\n",
    "        print(\"・\", end=\"\")\n",
    "        sound_data_list[i], sr = librosa.load(wav_data[i], sr=sampling_rate) #import as mono to match the training data\n",
    "    print(\"done!\")\n",
    "    return sound_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./audio/*.wav\"\n",
    "audio = load(path, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "csv = pd.read_csv(\"kitchen20.csv\", index_col=0)\n",
    "sorted_csv = csv.sort_values(by=\"path\").reset_index(drop=True)\n",
    "sorted_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sorted_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all = sorted_csv[\"category\"]\n",
    "label_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labeling\n",
    "# ラベル付\n",
    "def append(before, plus):\n",
    "    after = np.append(before, plus)\n",
    "    return after\n",
    "labeled = list(map(append, audio, label_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only \"frying-pan\" and \"chopping\" CHECK THIS WITH JOYA\n",
    "# 炒めると切る音だけを使う\n",
    "\n",
    "training_list = []\n",
    "for i in range(len(labeled)):\n",
    "    print(\".\", end=\"\")\n",
    "    if np.any((labeled[i] == \"frying-pan\") | (labeled[i] == \"chopping\") | (labeled[i] == \"boiling-water\")):\n",
    "        training_list.append(labeled[i])\n",
    "print(\"Done!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training_list))\n",
    "print(68 + 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listening\n",
    "\n",
    "listening_num = 150\n",
    "listening_data = training_list[listening_num][:-1]\n",
    "listening_label = training_list[listening_num][-1]\n",
    "print(listening_label)\n",
    "IPython.display.Audio(data=listening_data,rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ピッケルで保存\n",
    "## save training dataset by pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_pickle(path, name):\n",
    "    f = open(path, 'wb')\n",
    "    pickle.dump(name, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_pickle('./pickle_file/training_list.txt', training_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ピッケルで取得\n",
    "## get training dataset by pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def pickle_open(path):\n",
    "    f = open(path,\"rb\")\n",
    "    list_row = pickle.load(f)\n",
    "    return list_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_list = pickle_open('./pickle_file/training_list.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(data=training_list[88][:sr*5], rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. テストデータ読み込み\n",
    "## 2. Load test dataset\n",
    "- from youtube\n",
    "- test1: only cooking sound\n",
    "- test2: cooking sound and voice\n",
    "- test3: cooking sound, voice, and bgm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = \"./data/nikujaga.wav\"\n",
    "test1 = load(path1, sr)\n",
    "test1 = test1[0]\n",
    "\n",
    "path2 = \"./data/nikujaga_voice.wav\"\n",
    "test2 = load(path2, sr)\n",
    "test2 = test2[0]\n",
    "\n",
    "path3 = \"./data/nikujaga_voice_BGM.wav\"\n",
    "test3 = load(path3, sr)\n",
    "test3 = test3[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def devide(ndarray, time, sampling_rate):\n",
    "    devided_list = []\n",
    "    \n",
    "    for i in range(int((len(ndarray))//(time*sampling_rate))): \n",
    "        tmp = ndarray[time*sampling_rate*i : time*sampling_rate*(i+1)]\n",
    "        devided_list.append(tmp)\n",
    "\n",
    "    return devided_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============test1====================\n",
    "chopping1 = test1[sr*26: sr*55] # 28秒\n",
    "frying1 = test1[sr*85: sr*118] # 32秒\n",
    "simmering1 = test1[sr*118: sr*294] # 2分56秒\n",
    "\n",
    "chopping1_list = devide(chopping1, 5, sr)\n",
    "frying1_list = devide(frying1, 5, sr)\n",
    "simmering1_list = devide(simmering1, 5, sr)\n",
    "\n",
    "# ============test2====================\n",
    "chopping2_v1 = test2[sr*125: sr*139] # 13秒\n",
    "chopping2_v2 = test2[sr*146: sr*159] # 12秒\n",
    "chopping2_v3 = test2[sr*182: sr*191] # 8秒\n",
    "chopping2_v4 = test2[sr*206: sr*212] # 5秒\n",
    "chopping2_v5 = test2[sr*304: sr*311] # 6秒\n",
    "chopping2_v6 = test2[sr*315: sr*321] # 5秒\n",
    "frying2 = test2[sr*415: sr*491] # 76秒\n",
    "simmering2 = test2[sr*501: sr*814] # 312秒\n",
    "\n",
    "\n",
    "chopping2_list1 = devide(chopping2_v1, 5, sr)\n",
    "chopping2_list2 = devide(chopping2_v2, 5, sr)\n",
    "chopping2_list3 = devide(chopping2_v3, 5, sr)\n",
    "chopping2_list4 = devide(chopping2_v4, 5, sr)\n",
    "chopping2_list5 = devide(chopping2_v5, 5, sr)\n",
    "chopping2_list6 = devide(chopping2_v6, 5, sr)\n",
    "frying2_list = devide(frying2, 5, sr)\n",
    "simmering2_list = devide(simmering2, 5, sr)\n",
    "\n",
    "chopping2_list = np.concatenate([chopping2_list1, chopping2_list2, chopping2_list3, \n",
    "                           chopping2_list4, chopping2_list5, chopping2_list6], 0)\n",
    "frying2_list = frying2_list\n",
    "simmering2_list = simmering2_list\n",
    "\n",
    "# ============test3===============\n",
    "chopping3_v1 = test3[sr*7: sr*18] # 10秒\n",
    "chopping3_v2 = test3[sr*21: sr*29] # 7秒\n",
    "chopping3_v3 = test3[sr*29: sr*33] # 3秒\n",
    "frying3 = test3[sr*43: sr*61] # 17秒\n",
    "simmering3_v1 = test3[sr*62: sr*79] # 16秒\n",
    "simmering3_v2 = test3[sr*79: sr*107] # 27秒\n",
    "simmering3_v3 = test3[sr*123: sr*137] # 13秒\n",
    "\n",
    "chopping3_list1 = devide(chopping3_v1, 5, sr)\n",
    "chopping3_list2 = devide(chopping3_v2, 5, sr)\n",
    "chopping3_list3 = devide(chopping3_v3, 5, sr)\n",
    "frying3_list = devide(frying3, 5, sr)\n",
    "simmering3_list1 = devide(simmering3_v1, 5, sr)\n",
    "simmering3_list2 = devide(simmering3_v2, 5, sr)\n",
    "simmering3_list3 = devide(simmering3_v3, 5, sr)\n",
    "\n",
    "chopping3_list = np.concatenate([chopping3_list1, chopping3_list2#,chopping3_list3 #5秒ないのでカット\n",
    "                              ], 0)\n",
    "frying3_list = frying3_list\n",
    "simmering3_list = np.concatenate([simmering3_list1, simmering3_list2,simmering3_list3], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(data=chopping1_list[0], rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling(dataset_list, labeling_name):\n",
    "    labeled_list = []\n",
    "    for i in range(len(dataset_list)):\n",
    "        tmp = np.append(dataset_list[i], labeling_name)\n",
    "        labeled_list.append(tmp)\n",
    "        print(\".\", end=\"\")\n",
    "    print(\"Done!!\")\n",
    "    return labeled_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test1\n",
    "labeled_chopping1 = labeling(chopping1_list, \"chopping\")\n",
    "labeled_frying1 = labeling(frying1_list, \"frying-pan\")\n",
    "labeled_simmering1 = labeling(simmering1_list, \"simmering\")\n",
    "\n",
    "# test2\n",
    "labeled_chopping2 = labeling(chopping2_list, \"chopping\")\n",
    "labeled_frying2 = labeling(frying2_list, \"frying-pan\")\n",
    "labeled_simmering2 = labeling(simmering2_list, \"simmering\")\n",
    "\n",
    "# test3\n",
    "labeled_chopping3 = labeling(chopping3_list, \"chopping\")\n",
    "labeled_frying3 = labeling(frying3_list, \"frying-pan\")\n",
    "labeled_simmering3 = labeling(simmering3_list, \"simmering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1_list = labeled_chopping1 + labeled_frying1 + labeled_simmering1\n",
    "test2_list = labeled_chopping2 + labeled_frying2 + labeled_simmering2\n",
    "test3_list = labeled_chopping3 + labeled_frying3 + labeled_simmering3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ピッケルで保存\n",
    "## save test dataset by pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_pickle('./pickle_file/test1_list.txt', test1_list)\n",
    "#save_pickle('./pickle_file/test2_list.txt', test2_list)\n",
    "#save_pickle('./pickle_file/test3_list.txt', test3_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ピッケルで取得\n",
    "## get teat dataset by pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test1_list = pickle_open('./pickle_file/test1_list.txt')\n",
    "#test2_list = pickle_open('./pickle_file/test2_list.txt')\n",
    "#test3_list = pickle_open('./pickle_file/test3_list.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separation(labeled_list):\n",
    "    label_list = []\n",
    "    dataset_list = []\n",
    "    for i in range(len(labeled_list)):\n",
    "        tmp_label = labeled_list[i][-1]\n",
    "        tmp_dataset = labeled_list[i][:-1]\n",
    "        tmp_dataset = tmp_dataset.astype(float)\n",
    "        print(\".\", end=\"\")\n",
    "        label_list.append(tmp_label)\n",
    "        dataset_list.append(tmp_dataset)\n",
    "    print(\"Done!!\")\n",
    "    return dataset_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset, training_label = separation(training_list)\n",
    "test1_dataset, test1_label = separation(test1_list)\n",
    "test2_dataset, test2_label = separation(test2_list)\n",
    "test3_dataset, test3_label = separation(test3_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test1_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test1_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "listening_num = 0\n",
    "listening_data = training_dataset[listening_num]\n",
    "listening_label = training_label[listening_num]\n",
    "print(len(training_dataset))\n",
    "print(\"label:\",listening_label)\n",
    "IPython.display.Audio(data=listening_data,rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "listening_num = 24\n",
    "listening_data = test1_dataset[listening_num]\n",
    "listening_label = test1_label[listening_num]\n",
    "print(len(test1_dataset))\n",
    "print(\"label:\", listening_label)\n",
    "IPython.display.Audio(data=listening_data,rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listening_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. トレーニングデータのデータ拡張\n",
    "## 3. Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ホワイトノイズ加える\n",
    "# white_noise\n",
    "def add_white_noise(x_list, rate=0.002):\n",
    "    noise_list = []\n",
    "    for i in range(len(x_list)):\n",
    "        x = x_list[i]\n",
    "        noise_list.append(x + rate*np.random.randn(len(x)))\n",
    "    return noise_list\n",
    "\n",
    "# タイムシフト\n",
    "# time shift\n",
    "def shift_sound(x_list, rate=2):\n",
    "    shift_list = []\n",
    "    for i in range(len(x_list)):\n",
    "        x = x_list[i]\n",
    "        shift_list.append(np.roll(x, int(len(x)//rate)))\n",
    "    return shift_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_white = add_white_noise(training_dataset)\n",
    "training_shift = shift_sound(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_with_noise = training_dataset + training_white + training_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training_dataset_with_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3.5. フィルタリング + ブースチング\n",
    "## 3.5. Filtering + Boosting 城谷くん, this might be better after normalization? What do you think?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple Butterworth HPF (implemented with lfilter, but could also use filtfilt for zero phase shift)\n",
    "def hp_filt(s_list, cut_off_freq): #Probably smarter to use an array here, but this works\n",
    "    hp_filt_list = []\n",
    "    hpf = signal.butter(2, cut_off_freq, btype='highpass', fs=sr) \n",
    "    if type(s_list) is list:\n",
    "        for sig in s_list:\n",
    "            f_sig = signal.lfilter(hpf[0], hpf[1], sig)\n",
    "            hp_filt_list.append(f_sig) \n",
    "        return hp_filt_list\n",
    "    else: \n",
    "        sig=s_list\n",
    "        f_sig = signal.lfilter(hpf[0], hpf[1], sig)\n",
    "        hp_filt_list.append(f_sig)\n",
    "        #test this\n",
    "    return f_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple Butterworth LPF (implemented with lfilter, but could also use filtfilt for zero phase shift)\n",
    "def lp_filt(s_list, cut_off_freq): #Probably smarter to use an array here, but this works\n",
    "    lp_filt_list = []\n",
    "    lpf = signal.butter(2, cut_off_freq, btype='lowpass', fs=sr) \n",
    "    if type(s_list) is list:\n",
    "        for sig in s_list:\n",
    "            f_sig = signal.lfilter(lpf[0], lpf[1], sig)\n",
    "            lp_filt_list.append(f_sig) \n",
    "        return lp_filt_list\n",
    "    else: \n",
    "        sig=s_list\n",
    "        f_sig = signal.lfilter(lpf[0], lpf[1], sig)\n",
    "        lp_filt_list.append(f_sig)\n",
    "        #test this\n",
    "    return f_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IIR notch filter  (implemented with lfilter, but could also use filtfilt for zero phase shift)\n",
    "def filt_iir_notch(s_list, cut_off_freq): #Probably smarter to use an array here, but this works\n",
    "    iir_filt_list = []\n",
    "    iir = signal.iirnotch(cut_off_freq, 30, fs=sr) \n",
    "    if type(s_list) is list:\n",
    "        for sig in s_list:\n",
    "            f_sig = signal.lfilter(iir[0], iir[1], sig)\n",
    "            iir_filt_list.append(f_sig) \n",
    "        return iir_filt_list\n",
    "    else: \n",
    "        sig=s_list\n",
    "        f_sig = signal.lfilter(iir[0], iir[1], sig)\n",
    "        iir_filt_list.append(f_sig)\n",
    "        #test this\n",
    "    return f_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collect all data into a list\n",
    "full_data_list = training_list + test1_list + test2_list + test3_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into categories\n",
    "fp_list = []\n",
    "cp_list = []\n",
    "sm_list = []\n",
    "for entry in full_data_list:\n",
    "    if (entry[-1] == 'frying-pan' and len(entry[:-1])) == 220500:\n",
    "        fp_list.append(entry[:-1])\n",
    "    if entry[-1] == 'chopping'and len(entry[:-1]) == 220500:  #some of these are a strange size\n",
    "        cp_list.append(entry[:-1])\n",
    "    if entry[-1] == 'simmering'and len(entry[:-1]) == 220500:\n",
    "        sm_list.append(entry[:-1])\n",
    "        \n",
    "fp_array = np.array(fp_list, dtype = float)\n",
    "cp_array = np.array(cp_list, dtype = float)\n",
    "sm_array = np.array(sm_list, dtype = float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a single average signal to represent the category\n",
    "fp_mean_sig = fp_array.mean(axis = 0)\n",
    "cp_mean_sig = cp_array.mean(axis = 0)\n",
    "sm_mean_sig = sm_array.mean(axis = 0)\n",
    "\n",
    "mean_sig_list = [fp_mean_sig, cp_mean_sig, sm_mean_sig]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter parameters\n",
    "sig_l = 5 #signal length is 5s, 合ってる？\n",
    "cut_off_freq = sr/4\n",
    "N = sr * sig_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot average signals before filtering\n",
    "f_bins = fftfreq(N, 1 / sr)\n",
    "f_bins_pos = f_bins[:len(f_bins)//2] #-1 somewhere?\n",
    "plot_names = ['fp_mean_sig', 'cp_mean_sig', 'sm_mean_sig']\n",
    "name_var = 0\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for mean_sig in mean_sig_list:    \n",
    "    MEAN_SIG = fft(mean_sig)\n",
    "    MEAN_SIG_ABS = np.abs(MEAN_SIG)\n",
    "    MEAN_SIG_POS = 2*(MEAN_SIG_ABS[:len(MEAN_SIG_ABS)//2]) # *2 for amplitude makeup\n",
    "    MEAN_SIG_POS[0] = 0 # Remove DC component\n",
    "    ax.plot(f_bins_pos, MEAN_SIG_POS, label=plot_names[name_var])\n",
    "    name_var = name_var + 1\n",
    "    \n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Plot average signals after filtering\n",
    "hpf_mean_sig_list = hp_filt(mean_sig_list, cut_off_freq) #,axis=0 use this do to the whole array?)\n",
    "lpf_mean_sig_list = lp_filt(mean_sig_list, cut_off_freq) #,axis=0 use this do to the whole array?)\n",
    "iir_mean_sig_list = filt_iir_notch(mean_sig_list, cut_off_freq) #,axis=0 use this do to the whole array?)\n",
    "\n",
    "plot_names = ['fp_mean_sig_filt', 'cp_mean_sig_filt', 'sm_mean_sig_filt']\n",
    "name_var = 0\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for filt_mean_sig in hpf_mean_sig_list:    \n",
    "    FILT_MEAN_SIG = fft(filt_mean_sig)\n",
    "    FILT_MEAN_SIG_ABS = np.abs(FILT_MEAN_SIG)\n",
    "    FILT_MEAN_SIG_POS = 2*(FILT_MEAN_SIG_ABS[:len(FILT_MEAN_SIG_ABS)//2]) # *2 for amplitude makeup\n",
    "    FILT_MEAN_SIG_POS[0] = 0\n",
    "    ax.plot(f_bins_pos, FILT_MEAN_SIG_POS, label=plot_names[name_var])\n",
    "    name_var = name_var + 1\n",
    "    \n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_training_dataset_with_noise = lp_filt(training_dataset_with_noise, cut_off_freq)\n",
    "filtered_test1_dataset = hp_filt(test1_dataset, cut_off_freq)\n",
    "filtered_test2_dataset = hp_filt(test2_dataset, cut_off_freq)\n",
    "filtered_test3_dataset = hp_filt(test3_dataset, cut_off_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. 特徴量抽出\n",
    "## 4. Extract features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 正規化\n",
    "## normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(data_list):\n",
    "    for i in range(len(data_list)):\n",
    "        data_list[i] = librosa.util.normalize(data_list[i])\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_training_dataset = normalization(filtered_training_dataset_with_noise)\n",
    "normed_test1_dataset = normalization(filtered_test1_dataset)\n",
    "normed_test2_dataset = normalization(filtered_test2_dataset)\n",
    "normed_test3_dataset = normalization(filtered_test3_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特徴量取得\n",
    "## extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_feature(a_list,sr, MFCC=True, RMS=True, ZCR=True):\n",
    "    all_features = [] \n",
    "    feature_list = []\n",
    "  \n",
    "    for i in range(len(a_list)): \n",
    "        y = a_list[i]\n",
    "            \n",
    "        if MFCC:\n",
    "            mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "            for e in mfcc:\n",
    "                 feature_list += [np.mean(e),np.amax(e),np.amin(e), np.var(e)]\n",
    "        if RMS:\n",
    "            rms = librosa.feature.rms(y=y)\n",
    "            feature_list += [np.mean(rms),np.amax(rms),np.amin(rms),np.var(rms)]\n",
    "        if ZCR:\n",
    "            ZCR = librosa.feature.zero_crossing_rate(y=y)\n",
    "            feature_list += [np.mean(zcr),np.amax(zcr),np.amin(zcr),np.var(zcr)]\n",
    "\n",
    "        all_features.append(feature_list)\n",
    "        print(\".\",end=\"\")\n",
    "        feature_list = [] #初期化\n",
    "    print(\"end!\")\n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_training = make_feature(normed_training_dataset, sr, MFCC=True, RMS=False, ZCR=False)\n",
    "features_test1 = make_feature(normed_test1_dataset, sr, MFCC=True, RMS=False, ZCR=False)\n",
    "features_test2 = make_feature(normed_test2_dataset, sr, MFCC=True, RMS=False, ZCR=False)\n",
    "features_test3 = make_feature(normed_test3_dataset, sr, MFCC=True, RMS=False, ZCR=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データフレームに変換\n",
    "## make DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns\n",
    "sound_features = [#'ZCR', \n",
    "                  #'RMS',\n",
    "                  'MFCC']\n",
    "basic_features = ['mean', 'max', 'min', 'var']\n",
    "\n",
    "columns = ''\n",
    "for name in sound_features:\n",
    "    if name != 'MFCC':\n",
    "        for kind in basic_features:\n",
    "            columns += f\" {name}_{kind}\" #スペース区切りなのでスペースが大事\n",
    "    else:\n",
    "        for mfcc_number in range(1,21):\n",
    "            for kind in basic_features:\n",
    "                columns += f\" {name}{mfcc_number}_{kind}\"\n",
    "            \n",
    "#columns += ' label'     \n",
    "columns = columns.split() #区切り文字で分割する\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_DataFrame(features, columns, label):\n",
    "    df = pd.DataFrame(features)\n",
    "    df.columns = columns\n",
    "    df[\"label\"] = label\n",
    "    df['label'] = df['label'].map({'chopping':0, 'frying-pan':1, 'simmering':2})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = make_DataFrame(features_training, columns, training_label*3)\n",
    "df_training = df_training.fillna(2)\n",
    "df_test1 = make_DataFrame(features_test1, columns, test1_label)\n",
    "df_test2 = make_DataFrame(features_test2, columns, test2_label)\n",
    "df_test3 = make_DataFrame(features_test3, columns, test3_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 機械学習\n",
    "## 5. Machine learning\n",
    "- df_test1: only cooking sound\n",
    "- df_test2: cooking sound + voice\n",
    "- df_test3: cooking sound + voice + BGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "X_train = np.array(df_training.iloc[:, :-1], dtype = float)\n",
    "y_train = df_training[\"label\"]\n",
    "\n",
    "# test data\n",
    "# change data \n",
    "# df_test1 or df_test2 or df_test3\n",
    "X_test = np.array(df_test3.iloc[:, :-1], dtype = float)\n",
    "y_test = df_test3[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=15, random_state=0, n_estimators=15)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "# トレーニングデータに対する精度\n",
    "pred_train = clf.predict(X_train)\n",
    "\n",
    "accuracy_train = accuracy_score(y_train, pred_train)\n",
    "\n",
    "print('トレーニングデータに対する正解率： %.2f' % accuracy_train)\n",
    "    \n",
    "# テストデータに対する精度\n",
    "pred_test = clf.predict(X_test)\n",
    "\n",
    "accuracy_test = accuracy_score(y_test, pred_test)\n",
    "\n",
    "\n",
    "print('テストデータに対する正解率： %.3f' % accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cm1(X_test, y_test, model):\n",
    "    names = [\"chopping\",\"frying-pan\", \"simmering\"]\n",
    "    labels = [0, 1, 2]\n",
    "    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "#最初\n",
    "    ax1 = plt.subplot()\n",
    "    cm1 = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "    sns.heatmap(cm1, annot = True, cmap='Blues', fmt =\".0f\",ax=ax1,cbar=True,square=True,\n",
    "               xticklabels=names,\n",
    "               yticklabels=names)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "\n",
    "\n",
    "    from sklearn.metrics import classification_report\n",
    "    \n",
    "    print(classification_report(y_test, y_pred, target_names=names))\n",
    "    #plt.savefig('seaborn_heatmap_living_1.pdf',bbox_inches='tight')\n",
    "\n",
    "def cm2(X_test, y_test, model):\n",
    "    names = [\"chopping\",\"frying-pan\", \"simmering\"]\n",
    "    labels = [0, 1, 2]\n",
    "    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "#いじる\n",
    "    ax2 = plt.subplot()\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "    normed_c = (cm.T *100/ cm.astype(np.float).sum(axis=1)).T\n",
    "    sns.heatmap(normed_c, annot = True, cmap='Blues', fmt =\".1f\",ax=ax2,cbar=False,square=True,\n",
    "               xticklabels=names,\n",
    "               yticklabels=names)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    #plt.savefig('seaborn_heatmap_bedroom_2.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm1(X_test, y_test, clf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
